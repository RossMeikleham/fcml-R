```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

<h3>Cross-validation</h3>
<p>In this notebook, we perform a cross-validation to try and choose the
polynomial order for some synthetic data.</p>
<p>The loss (in our case, this is equal to the squared error) on the training
data will always decrease as we make the model more complex, and therefore
cannot be used to select how complex the model ought to be. In some cases we may
be lucky enough to have an independent test set for model validation but often
we will not. In a cross-validation procedure (CV) we split the data into $K$
folds, and train $K$ different models, each with a different data fold removed.
This removed fold is used for testing and the performance is averaged over the
$K$ different folds.</p>
<p>This process is illustrated in the following figure:</p>
<img src="cvdiagram.png">
<p>We start by generating two datasets - one on which we will perform the CV,
and a second independent test set.</p>

```{r}
N <- 100
x <- 10 * runif(N, 0, 1) - 5
t <- cbind(5*x**3 - x**2 + x + 150 * rnorm(N, mean=0, sd=1))
N_test <- 200
x_test <- seq(-5, 5, length=N_test)
t_test <- 5*x_test**3 - x_test**2 + x_test + 150 * rnorm(N_test, mean=0, sd=1)
```

<p>We now loop over model order (from 0th up to 10th order polynomials) and for
each order, perform a $K=10$ fold CV. For each fold-order combintaion we compute
the mean squared loss on the training data, the mean squared loss on the held
out fold and the mean squared loss on the independent test set.</p>
<p>Note that in many cases, it would be sensible to randomise the order of the
data before placing it into the folds.</p>

```{r}
max_order <- 10
K <- 10
sizes <- rep(floor(N/10), K)
sizes[length(sizes)] <- sizes[length(sizes)] + N - sum(sizes)
c_sizes <- c(0, cumsum(sizes))
X <- cbind(rep(1, length(x)))
X_test <- cbind(rep(1, length(x_test)))
cv_loss <- matrix(0, nrow=K, ncol=max_order + 1)
ind_loss <- matrix(0, nrow=K, ncol=max_order + 1)
train_loss <- matrix(0, nrow=K, ncol=max_order + 1)
    
for (k in 1:(max_order+1)) {
    for (fold in 1:K) {
        fold_slice <- c_sizes[fold]:c_sizes[fold+1]
        rows_remove <- seq(c_sizes[fold]+1, c_sizes[fold+1], by=1)
        
        X_fold <- X[fold_slice,]
        X_train <- X[-rows_remove,]
        t_fold <- t[fold_slice,]
        t_train <- t[-rows_remove,]
        
        w <- solve(t(X_train) %*% X_train, t(X_train) %*% t_train)
        fold_pred <- X_fold %*% w
        cv_loss[fold,k] <- mean((fold_pred - t_fold)**2)
        ind_pred <- X_test %*% w
        ind_loss[fold,k] <- mean((ind_pred - t_test)**2)
        train_pred <- X_train %*% w
        train_loss[fold,k] <- mean((train_pred - t_train)**2)
    }
    X <- cbind(X, x**k)
    X_test <- cbind(X_test, x_test**k)
}
```

<p>Finally, we plot the mean performance in each case (where the mean is taken
over folds).</p>

```{r}
order <- seq(0, max_order, by=1)

plot(order, colMeans(train_loss), type="l", col="blue", xlab="Model order", 
     ylab="Mean squared loss", xlim=c(0,10))

lines(order, colMeans(cv_loss), col="red")
lines(order, colMeans(ind_loss), col="green")    

legend('topright', pch=c(15, 15, 15), col=c("blue", "red", "green"), 
       legend=c("Training loss", "CV loss", "Independent test loss"))
```

<p>As expected, training loss (blue) always decreaes. The CV loss doesn't, and
comes to a minimum at around 3rd order (this will not always be the case - it
depends on the dataset generated at the start of the notebook). The independent
loss typically has a more well-defined minimum.</p>
